use std::collections::{HashMap, VecDeque};
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use serde::{Serialize, Deserialize};
use tracing::{info, warn, error, debug};

/// æ™ºèƒ½å†…å­˜ä¼˜åŒ–å™¨ - å†…å­˜æ± ã€ç¼“å­˜ä¼˜åŒ–å’Œåƒåœ¾å›æ”¶ä¼˜åŒ–
#[derive(Clone)]
pub struct MemoryOptimizer {
    config: MemoryConfig,
    memory_pools: Arc<Mutex<HashMap<String, MemoryPool>>>,
    cache_manager: Arc<CacheManager>,
    allocation_tracker: Arc<Mutex<AllocationTracker>>,
    memory_pressure_monitor: Arc<Mutex<MemoryPressureMonitor>>,
}

/// å†…å­˜ä¼˜åŒ–é…ç½®
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct MemoryConfig {
    pub max_heap_size_mb: usize,
    pub cache_size_mb: usize,
    pub pool_sizes: HashMap<String, usize>,
    pub gc_trigger_threshold: f64,
    pub memory_pressure_threshold: f64,
    pub prefetch_enabled: bool,
    pub compression_enabled: bool,
    pub numa_aware_allocation: bool,
    pub zero_copy_optimizations: bool,
}

impl Default for MemoryConfig {
    fn default() -> Self {
        let mut pool_sizes = HashMap::new();
        pool_sizes.insert("small".to_string(), 64);      // 64 bytes
        pool_sizes.insert("medium".to_string(), 1024);   // 1KB
        pool_sizes.insert("large".to_string(), 8192);    // 8KB
        pool_sizes.insert("huge".to_string(), 65536);    // 64KB
        
        Self {
            max_heap_size_mb: 4096,
            cache_size_mb: 512,
            pool_sizes,
            gc_trigger_threshold: 80.0,
            memory_pressure_threshold: 85.0,
            prefetch_enabled: true,
            compression_enabled: true,
            numa_aware_allocation: true,
            zero_copy_optimizations: true,
        }
    }
}

/// å†…å­˜æ± 
#[derive(Debug)]
pub struct MemoryPool {
    pool_name: String,
    chunk_size: usize,
    available_chunks: VecDeque<MemoryChunk>,
    allocated_chunks: HashMap<u64, MemoryChunk>,
    total_allocated: usize,
    peak_allocated: usize,
    allocation_count: u64,
    deallocation_count: u64,
    fragmentation_ratio: f64,
}

/// å†…å­˜å—
#[derive(Debug, Clone)]
pub struct MemoryChunk {
    id: u64,
    size: usize,
    allocated_at: Instant,
    last_accessed: Instant,
    access_count: u64,
    numa_node: Option<usize>,
    compressed: bool,
}

/// ç¼“å­˜ç®¡ç†å™¨
#[derive(Debug)]
pub struct CacheManager {
    l1_cache: LruCache<String, CacheEntry>,
    l2_cache: LruCache<String, CacheEntry>,
    cache_stats: CacheStatistics,
    prefetch_predictor: PrefetchPredictor,
    compression_engine: CompressionEngine,
}

/// LRUç¼“å­˜å®ç°
#[derive(Debug)]
pub struct LruCache<K, V> {
    capacity: usize,
    items: HashMap<K, (V, u64)>,
    access_order: VecDeque<(K, u64)>,
    access_counter: u64,
}

/// ç¼“å­˜æ¡ç›®
#[derive(Debug, Clone)]
pub struct CacheEntry {
    data: Vec<u8>,
    created_at: Instant,
    last_accessed: Instant,
    access_count: u64,
    size: usize,
    compressed: bool,
    hit_rate: f64,
}

/// ç¼“å­˜ç»Ÿè®¡
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CacheStatistics {
    pub l1_hits: u64,
    pub l1_misses: u64,
    pub l2_hits: u64,
    pub l2_misses: u64,
    pub evictions: u64,
    pub total_size: usize,
    pub hit_rate: f64,
}

/// é¢„å–é¢„æµ‹å™¨
#[derive(Debug)]
pub struct PrefetchPredictor {
    access_patterns: HashMap<String, AccessPattern>,
    prediction_accuracy: f64,
    enabled: bool,
}

#[derive(Debug, Clone)]
pub struct AccessPattern {
    key: String,
    sequence: VecDeque<String>,
    frequency: f64,
    last_updated: Instant,
    prediction_success: u64,
    prediction_total: u64,
}

/// å‹ç¼©å¼•æ“
#[derive(Debug)]
pub struct CompressionEngine {
    algorithm: CompressionAlgorithm,
    compression_ratio: f64,
    enabled: bool,
    threshold_size: usize,
}

#[derive(Debug, Clone, Copy)]
pub enum CompressionAlgorithm {
    Lz4,
    Zstd,
    Snappy,
    None,
}

/// åˆ†é…è¿½è¸ªå™¨
#[derive(Debug)]
pub struct AllocationTracker {
    allocations: HashMap<u64, AllocationInfo>,
    allocation_history: VecDeque<AllocationEvent>,
    memory_usage_timeline: VecDeque<MemoryUsagePoint>,
    leak_detector: LeakDetector,
    fragmentation_analyzer: FragmentationAnalyzer,
}

#[derive(Debug, Clone)]
pub struct AllocationInfo {
    id: u64,
    size: usize,
    allocated_at: Instant,
    allocation_type: AllocationType,
    call_stack: Vec<String>,
    numa_node: Option<usize>,
}

#[derive(Debug, Clone, Copy)]
pub enum AllocationType {
    Pool,
    Heap,
    Stack,
    Mmap,
    Cache,
}

#[derive(Debug, Clone)]
pub struct AllocationEvent {
    timestamp: Instant,
    event_type: AllocationEventType,
    size: usize,
    pool_name: Option<String>,
}

#[derive(Debug, Clone)]
pub enum AllocationEventType {
    Allocate,
    Deallocate,
    Reallocate,
    Free,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryUsagePoint {
    timestamp: u64,
    total_allocated: usize,
    heap_usage: usize,
    cache_usage: usize,
    pool_usage: HashMap<String, usize>,
    fragmentation: f64,
    pressure: f64,
}

/// æ³„éœ²æ£€æµ‹å™¨
#[derive(Debug)]
pub struct LeakDetector {
    suspected_leaks: HashMap<u64, LeakCandidate>,
    threshold_duration: Duration,
    threshold_size: usize,
    enabled: bool,
}

#[derive(Debug, Clone)]
pub struct LeakCandidate {
    allocation_id: u64,
    size: usize,
    allocated_at: Instant,
    last_accessed: Instant,
    access_count: u64,
    leak_probability: f64,
}

/// ç¢ç‰‡åˆ†æå™¨
#[derive(Debug)]
pub struct FragmentationAnalyzer {
    free_blocks: VecDeque<FreeBlock>,
    fragmentation_ratio: f64,
    compaction_threshold: f64,
    last_compaction: Instant,
}

#[derive(Debug, Clone)]
pub struct FreeBlock {
    address: u64,
    size: usize,
    freed_at: Instant,
}

/// å†…å­˜å‹åŠ›ç›‘æ§å™¨
#[derive(Debug)]
pub struct MemoryPressureMonitor {
    current_pressure: f64,
    pressure_history: VecDeque<f64>,
    gc_recommendations: VecDeque<GcRecommendation>,
    memory_alerts: VecDeque<MemoryAlert>,
}

#[derive(Debug, Clone)]
pub struct GcRecommendation {
    reason: String,
    urgency: GcUrgency,
    estimated_freed_bytes: usize,
    created_at: Instant,
}

#[derive(Debug, Clone, Copy)]
pub enum GcUrgency {
    Low,
    Medium,
    High,
    Critical,
}

#[derive(Debug, Clone)]
pub struct MemoryAlert {
    level: AlertLevel,
    message: String,
    memory_usage: usize,
    threshold: usize,
    timestamp: Instant,
}

#[derive(Debug, Clone, Copy)]
pub enum AlertLevel {
    Info,
    Warning,
    Critical,
    Emergency,
}

impl MemoryOptimizer {
    /// åˆ›å»ºæ–°çš„å†…å­˜ä¼˜åŒ–å™¨
    pub fn new(config: MemoryConfig) -> Self {
        info!("ğŸ§  åˆå§‹åŒ–æ™ºèƒ½å†…å­˜ä¼˜åŒ–å™¨");
        info!("ğŸ”§ é…ç½®: æœ€å¤§å †={}MB, ç¼“å­˜={}MB, æ± æ•°={}", 
              config.max_heap_size_mb, config.cache_size_mb, config.pool_sizes.len());
        
        let mut memory_pools = HashMap::new();
        
        // åˆå§‹åŒ–å†…å­˜æ± 
        for (pool_name, chunk_size) in &config.pool_sizes {
            let pool = MemoryPool {
                pool_name: pool_name.clone(),
                chunk_size: *chunk_size,
                available_chunks: VecDeque::new(),
                allocated_chunks: HashMap::new(),
                total_allocated: 0,
                peak_allocated: 0,
                allocation_count: 0,
                deallocation_count: 0,
                fragmentation_ratio: 0.0,
            };
            memory_pools.insert(pool_name.clone(), pool);
        }

        let cache_manager = Arc::new(CacheManager {
            l1_cache: LruCache::new(config.cache_size_mb / 4), // L1 ç¼“å­˜å 1/4
            l2_cache: LruCache::new(config.cache_size_mb * 3 / 4), // L2 ç¼“å­˜å 3/4
            cache_stats: CacheStatistics {
                l1_hits: 0,
                l1_misses: 0,
                l2_hits: 0,
                l2_misses: 0,
                evictions: 0,
                total_size: 0,
                hit_rate: 0.0,
            },
            prefetch_predictor: PrefetchPredictor {
                access_patterns: HashMap::new(),
                prediction_accuracy: 0.0,
                enabled: config.prefetch_enabled,
            },
            compression_engine: CompressionEngine {
                algorithm: CompressionAlgorithm::Lz4,
                compression_ratio: 0.7, // å¹³å‡70%å‹ç¼©æ¯”
                enabled: config.compression_enabled,
                threshold_size: 1024, // 1KBä»¥ä¸Šæ‰å‹ç¼©
            },
        });

        Self {
            config,
            memory_pools: Arc::new(Mutex::new(memory_pools)),
            cache_manager,
            allocation_tracker: Arc::new(Mutex::new(AllocationTracker {
                allocations: HashMap::new(),
                allocation_history: VecDeque::new(),
                memory_usage_timeline: VecDeque::new(),
                leak_detector: LeakDetector {
                    suspected_leaks: HashMap::new(),
                    threshold_duration: Duration::from_secs(300), // 5åˆ†é’Ÿ
                    threshold_size: 1024 * 1024, // 1MB
                    enabled: true,
                },
                fragmentation_analyzer: FragmentationAnalyzer {
                    free_blocks: VecDeque::new(),
                    fragmentation_ratio: 0.0,
                    compaction_threshold: 0.3, // 30%ç¢ç‰‡ç‡è§¦å‘æ•´ç†
                    last_compaction: Instant::now(),
                },
            })),
            memory_pressure_monitor: Arc::new(Mutex::new(MemoryPressureMonitor {
                current_pressure: 0.0,
                pressure_history: VecDeque::new(),
                gc_recommendations: VecDeque::new(),
                memory_alerts: VecDeque::new(),
            })),
        }
    }

    /// å¯åŠ¨å†…å­˜ä¼˜åŒ–å™¨
    pub async fn start(&self) {
        info!("ğŸš€ å¯åŠ¨å†…å­˜ä¼˜åŒ–å™¨");
        
        // å¯åŠ¨å†…å­˜å‹åŠ›ç›‘æ§
        self.start_memory_pressure_monitoring().await;
        
        // å¯åŠ¨å†…å­˜æ³„éœ²æ£€æµ‹
        self.start_leak_detection().await;
        
        // å¯åŠ¨å†…å­˜æ•´ç†ä»»åŠ¡
        self.start_memory_compaction().await;
        
        // å¯åŠ¨é¢„å–ä¼˜åŒ–
        if self.config.prefetch_enabled {
            self.start_prefetch_optimization().await;
        }
        
        info!("âœ… å†…å­˜ä¼˜åŒ–å™¨å¯åŠ¨å®Œæˆ");
    }

    /// ä»å†…å­˜æ± åˆ†é…å†…å­˜
    pub fn allocate_from_pool(&self, pool_name: &str, size: usize) -> Result<u64, MemoryError> {
        let mut pools = self.memory_pools.lock().unwrap();
        let pool = pools.get_mut(pool_name)
            .ok_or_else(|| MemoryError::PoolNotFound(pool_name.to_string()))?;

        // æ£€æŸ¥å¤§å°æ˜¯å¦åŒ¹é…
        if size > pool.chunk_size {
            return Err(MemoryError::SizeExceedsChunkSize(size, pool.chunk_size));
        }

        let chunk_id = pool.allocation_count;
        let chunk = if let Some(mut chunk) = pool.available_chunks.pop_front() {
            // é‡ç”¨å¯ç”¨å—
            chunk.id = chunk_id;
            chunk.allocated_at = Instant::now();
            chunk.last_accessed = Instant::now();
            chunk.access_count = 0;
            chunk
        } else {
            // åˆ›å»ºæ–°å—
            MemoryChunk {
                id: chunk_id,
                size: pool.chunk_size,
                allocated_at: Instant::now(),
                last_accessed: Instant::now(),
                access_count: 0,
                numa_node: self.select_optimal_numa_node(),
                compressed: false,
            }
        };

        pool.allocated_chunks.insert(chunk_id, chunk);
        pool.allocation_count += 1;
        pool.total_allocated += pool.chunk_size;
        pool.peak_allocated = pool.peak_allocated.max(pool.total_allocated);

        // è®°å½•åˆ†é…äº‹ä»¶
        self.track_allocation(chunk_id, pool.chunk_size, AllocationType::Pool, Some(pool_name.to_string()));

        debug!("ğŸ§Š ä»æ±  {} åˆ†é…å†…å­˜å—: ID={}, å¤§å°={}bytes", pool_name, chunk_id, pool.chunk_size);
        Ok(chunk_id)
    }

    /// é‡Šæ”¾å†…å­˜æ± ä¸­çš„å†…å­˜
    pub fn deallocate_from_pool(&self, pool_name: &str, chunk_id: u64) -> Result<(), MemoryError> {
        let mut pools = self.memory_pools.lock().unwrap();
        let pool = pools.get_mut(pool_name)
            .ok_or_else(|| MemoryError::PoolNotFound(pool_name.to_string()))?;

        let chunk = pool.allocated_chunks.remove(&chunk_id)
            .ok_or(MemoryError::ChunkNotFound(chunk_id))?;

        // å°†å—è¿”å›åˆ°å¯ç”¨æ± 
        pool.available_chunks.push_back(chunk);
        pool.deallocation_count += 1;
        pool.total_allocated -= pool.chunk_size;

        // è®°å½•é‡Šæ”¾äº‹ä»¶
        self.track_deallocation(chunk_id, pool.chunk_size);

        debug!("ğŸ”„ é‡Šæ”¾å†…å­˜å—åˆ°æ±  {}: ID={}", pool_name, chunk_id);
        Ok(())
    }

    /// ç¼“å­˜æ•°æ®
    pub async fn cache_put(&self, key: String, data: Vec<u8>) -> Result<(), MemoryError> {
        let cache_manager = Arc::clone(&self.cache_manager);
        let data_size = data.len();
        
        // å†³å®šç¼“å­˜çº§åˆ«
        let use_l1 = data_size < 4096; // 4KBä»¥ä¸‹ç”¨L1ç¼“å­˜
        
        let mut cache_entry = CacheEntry {
            data: data.clone(),
            created_at: Instant::now(),
            last_accessed: Instant::now(),
            access_count: 0,
            size: data_size,
            compressed: false,
            hit_rate: 0.0,
        };

        // å‹ç¼©å¤§æ•°æ®
        if cache_manager.compression_engine.enabled && data_size > cache_manager.compression_engine.threshold_size {
            cache_entry.data = self.compress_data(&data)?;
            cache_entry.compressed = true;
            debug!("ğŸ—œï¸ ç¼“å­˜æ•°æ®å·²å‹ç¼©: {} -> {} bytes", data_size, cache_entry.data.len());
        }

        // æ”¾å…¥é€‚å½“çš„ç¼“å­˜å±‚
        tokio::task::spawn_blocking(move || {
            let _ = use_l1; // æ ‡è®°ä¸ºå·²ä½¿ç”¨é¿å…è­¦å‘Š
            // TODO: å®ç°å¼‚æ­¥ç¼“å­˜æ“ä½œ
        }).await.unwrap();

        debug!("ğŸ“¦ ç¼“å­˜æ•°æ®: key={}, size={}bytes, compressed={}", key, data_size, cache_entry.compressed);
        Ok(())
    }

    /// ä»ç¼“å­˜è·å–æ•°æ®
    pub async fn cache_get(&self, key: &str) -> Option<Vec<u8>> {
        debug!("ğŸ” æŸ¥è¯¢ç¼“å­˜: key={}", key);
        
        // è®°å½•è®¿é—®æ¨¡å¼ç”¨äºé¢„å–
        self.record_access_pattern(key).await;
        
        // TODO: å®ç°å®é™…çš„ç¼“å­˜æŸ¥è¯¢
        // è¿™é‡Œç®€åŒ–å®ç°
        None
    }

    /// å‹ç¼©æ•°æ®
    fn compress_data(&self, data: &[u8]) -> Result<Vec<u8>, MemoryError> {
        // ç®€åŒ–çš„å‹ç¼©å®ç°
        let compressed_size = (data.len() as f64 * 0.7) as usize; // æ¨¡æ‹Ÿ70%å‹ç¼©æ¯”
        let compressed_data = vec![0u8; compressed_size];
        Ok(compressed_data)
    }

    /// è§£å‹æ•°æ®
    fn decompress_data(&self, compressed_data: &[u8]) -> Result<Vec<u8>, MemoryError> {
        // ç®€åŒ–çš„è§£å‹å®ç°
        let original_size = (compressed_data.len() as f64 / 0.7) as usize;
        let decompressed_data = vec![0u8; original_size];
        Ok(decompressed_data)
    }

    /// è®°å½•è®¿é—®æ¨¡å¼
    async fn record_access_pattern(&self, key: &str) {
        // TODO: å®ç°è®¿é—®æ¨¡å¼è®°å½•ï¼Œç”¨äºé¢„å–ä¼˜åŒ–
        debug!("ğŸ“Š è®°å½•è®¿é—®æ¨¡å¼: key={}", key);
    }

    /// é€‰æ‹©æœ€ä¼˜NUMAèŠ‚ç‚¹
    fn select_optimal_numa_node(&self) -> Option<usize> {
        if !self.config.numa_aware_allocation {
            return None;
        }
        
        // ç®€åŒ–çš„NUMAèŠ‚ç‚¹é€‰æ‹©
        let cpu_count = num_cpus::get();
        let numa_nodes = std::cmp::max(1, cpu_count / 4);
        Some(fastrand::usize(0..numa_nodes))
    }

    /// è®°å½•åˆ†é…äº‹ä»¶
    fn track_allocation(&self, id: u64, size: usize, alloc_type: AllocationType, pool_name: Option<String>) {
        let mut tracker = self.allocation_tracker.lock().unwrap();
        
        let allocation_info = AllocationInfo {
            id,
            size,
            allocated_at: Instant::now(),
            allocation_type: alloc_type,
            call_stack: vec![], // ç®€åŒ–å®ç°
            numa_node: self.select_optimal_numa_node(),
        };
        
        tracker.allocations.insert(id, allocation_info);
        
        let event = AllocationEvent {
            timestamp: Instant::now(),
            event_type: AllocationEventType::Allocate,
            size,
            pool_name,
        };
        
        tracker.allocation_history.push_back(event);
        
        // ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if tracker.allocation_history.len() > 10000 {
            tracker.allocation_history.pop_front();
        }
        
        debug!("ğŸ“ è®°å½•åˆ†é…: ID={}, å¤§å°={}bytes, ç±»å‹={:?}", id, size, alloc_type);
    }

    /// è®°å½•é‡Šæ”¾äº‹ä»¶
    fn track_deallocation(&self, id: u64, size: usize) {
        let mut tracker = self.allocation_tracker.lock().unwrap();
        
        tracker.allocations.remove(&id);
        
        let event = AllocationEvent {
            timestamp: Instant::now(),
            event_type: AllocationEventType::Deallocate,
            size,
            pool_name: None,
        };
        
        tracker.allocation_history.push_back(event);
        
        debug!("ğŸ—‘ï¸ è®°å½•é‡Šæ”¾: ID={}, å¤§å°={}bytes", id, size);
    }

    /// å¯åŠ¨å†…å­˜å‹åŠ›ç›‘æ§
    async fn start_memory_pressure_monitoring(&self) {
        let monitor = Arc::clone(&self.memory_pressure_monitor);
        let config = self.config.clone();
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(1));
            
            loop {
                interval.tick().await;
                Self::monitor_memory_pressure(monitor.clone(), &config).await;
            }
        });
    }

    /// ç›‘æ§å†…å­˜å‹åŠ›
    async fn monitor_memory_pressure(monitor: Arc<Mutex<MemoryPressureMonitor>>, config: &MemoryConfig) {
        let mut monitor_guard = monitor.lock().unwrap();
        
        // è®¡ç®—å½“å‰å†…å­˜å‹åŠ›ï¼ˆæ¨¡æ‹Ÿï¼‰
        let current_usage = Self::get_current_memory_usage();
        let pressure = (current_usage as f64 / (config.max_heap_size_mb * 1024 * 1024) as f64) * 100.0;
        
        monitor_guard.current_pressure = pressure;
        monitor_guard.pressure_history.push_back(pressure);
        
        // ä¿æŒå†å²è®°å½•åœ¨1å°æ—¶å†…
        if monitor_guard.pressure_history.len() > 3600 {
            monitor_guard.pressure_history.pop_front();
        }
        
        // æ£€æŸ¥æ˜¯å¦éœ€è¦GC
        if pressure > config.gc_trigger_threshold {
            let recommendation = GcRecommendation {
                reason: format!("å†…å­˜å‹åŠ›è¾¾åˆ°{:.1}%ï¼Œè¶…è¿‡é˜ˆå€¼{:.1}%", pressure, config.gc_trigger_threshold),
                urgency: if pressure > 95.0 {
                    GcUrgency::Critical
                } else if pressure > 90.0 {
                    GcUrgency::High
                } else {
                    GcUrgency::Medium
                },
                estimated_freed_bytes: (current_usage as f64 * 0.3) as usize, // ä¼°è®¡å¯é‡Šæ”¾30%
                created_at: Instant::now(),
            };
            
            monitor_guard.gc_recommendations.push_back(recommendation);
            warn!("âš ï¸ å†…å­˜å‹åŠ›è¿‡é«˜: {:.1}%ï¼Œå»ºè®®è¿›è¡Œåƒåœ¾å›æ”¶", pressure);
        }
        
        // ç”Ÿæˆå†…å­˜å‘Šè­¦
        if pressure > config.memory_pressure_threshold {
            let alert = MemoryAlert {
                level: if pressure > 95.0 {
                    AlertLevel::Critical
                } else if pressure > 90.0 {
                    AlertLevel::Warning
                } else {
                    AlertLevel::Info
                },
                message: format!("å†…å­˜å‹åŠ›: {pressure:.1}%"),
                memory_usage: current_usage,
                threshold: (config.memory_pressure_threshold / 100.0 * (config.max_heap_size_mb * 1024 * 1024) as f64) as usize,
                timestamp: Instant::now(),
            };
            
            monitor_guard.memory_alerts.push_back(alert);
        }
        
        debug!("ğŸ“Š å†…å­˜å‹åŠ›ç›‘æ§: {:.1}%", pressure);
    }

    /// è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡
    fn get_current_memory_usage() -> usize {
        // è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„å†…å­˜ä½¿ç”¨é‡è·å–
        // ç®€åŒ–å®ç°è¿”å›æ¨¡æ‹Ÿå€¼
        1024 * 1024 * (500 + fastrand::usize(0..1000)) // 500MB-1.5GB
    }

    /// å¯åŠ¨å†…å­˜æ³„éœ²æ£€æµ‹
    async fn start_leak_detection(&self) {
        let tracker = Arc::clone(&self.allocation_tracker);
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(60)); // æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            
            loop {
                interval.tick().await;
                Self::detect_memory_leaks(tracker.clone()).await;
            }
        });
    }

    /// æ£€æµ‹å†…å­˜æ³„éœ²
    async fn detect_memory_leaks(tracker: Arc<Mutex<AllocationTracker>>) {
        let now = Instant::now();
        let mut leak_candidates = Vec::new();
        
        // åˆ†ä¸¤æ­¥å¤„ç†ï¼Œå…ˆæ”¶é›†å¯èƒ½çš„æ³„éœ²å€™é€‰ï¼Œå†æ’å…¥
        {
            let tracker_guard = tracker.lock().unwrap();
            
            for (id, allocation) in &tracker_guard.allocations {
                let age = now.duration_since(allocation.allocated_at);
                
                // æ£€æŸ¥é•¿æœŸæœªè®¿é—®çš„å¤§å†…å­˜å—
                if age > tracker_guard.leak_detector.threshold_duration &&
                   allocation.size > tracker_guard.leak_detector.threshold_size {
                    
                    let leak_probability = Self::calculate_leak_probability(allocation, age);
                    
                    let leak_candidate = LeakCandidate {
                        allocation_id: *id,
                        size: allocation.size,
                        allocated_at: allocation.allocated_at,
                        last_accessed: allocation.allocated_at, // ç®€åŒ–
                        access_count: 0, // ç®€åŒ–
                        leak_probability,
                    };
                    
                    leak_candidates.push((*id, leak_candidate, leak_probability));
                }
            }
        }
        
        // ç°åœ¨æ’å…¥æ³„éœ²å€™é€‰å¹¶è®°å½•æ—¥å¿—
        {
            let mut tracker_guard = tracker.lock().unwrap();
            for (id, leak_candidate, leak_probability) in leak_candidates {
                tracker_guard.leak_detector.suspected_leaks.insert(id, leak_candidate.clone());
                
                if leak_probability > 0.8 {
                    warn!("ğŸš¨ å¯èƒ½çš„å†…å­˜æ³„éœ²: ID={}, å¤§å°={}bytes, æ¦‚ç‡={:.1}%",
                          id, leak_candidate.size, leak_probability * 100.0);
                }
            }
            
            debug!("ğŸ” å†…å­˜æ³„éœ²æ£€æµ‹å®Œæˆï¼Œå‘ç° {} ä¸ªç–‘ä¼¼æ³„éœ²", 
                   tracker_guard.leak_detector.suspected_leaks.len());
        }
    }

    /// è®¡ç®—æ³„éœ²æ¦‚ç‡
    fn calculate_leak_probability(allocation: &AllocationInfo, age: Duration) -> f64 {
        // ç®€åŒ–çš„æ³„éœ²æ¦‚ç‡è®¡ç®—
        let age_factor = (age.as_secs() as f64 / 3600.0).min(1.0); // 1å°æ—¶å†…çº¿æ€§å¢é•¿
        let size_factor = (allocation.size as f64 / (1024.0 * 1024.0)).min(1.0); // 1MBä»¥ä¸Šçº¿æ€§å¢é•¿
        
        (age_factor * 0.5 + size_factor * 0.3 + 0.2).min(1.0)
    }

    /// å¯åŠ¨å†…å­˜æ•´ç†
    async fn start_memory_compaction(&self) {
        let tracker = Arc::clone(&self.allocation_tracker);
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(300)); // æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            
            loop {
                interval.tick().await;
                Self::perform_memory_compaction(tracker.clone()).await;
            }
        });
    }

    /// æ‰§è¡Œå†…å­˜æ•´ç†
    async fn perform_memory_compaction(tracker: Arc<Mutex<AllocationTracker>>) {
        let mut tracker_guard = tracker.lock().unwrap();
        let fragmentation = tracker_guard.fragmentation_analyzer.fragmentation_ratio;
        
        if fragmentation > tracker_guard.fragmentation_analyzer.compaction_threshold {
            info!("ğŸ”§ å¼€å§‹å†…å­˜æ•´ç†ï¼Œç¢ç‰‡ç‡: {:.1}%", fragmentation * 100.0);
            
            // æ¨¡æ‹Ÿå†…å­˜æ•´ç†è¿‡ç¨‹
            tracker_guard.fragmentation_analyzer.free_blocks.clear();
            tracker_guard.fragmentation_analyzer.fragmentation_ratio = fragmentation * 0.3; // å‡å°‘70%ç¢ç‰‡
            tracker_guard.fragmentation_analyzer.last_compaction = Instant::now();
            
            info!("âœ… å†…å­˜æ•´ç†å®Œæˆï¼Œæ–°ç¢ç‰‡ç‡: {:.1}%", 
                  tracker_guard.fragmentation_analyzer.fragmentation_ratio * 100.0);
        }
    }

    /// å¯åŠ¨é¢„å–ä¼˜åŒ–
    async fn start_prefetch_optimization(&self) {
        let _cache_manager = Arc::clone(&self.cache_manager);
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(10));
            
            loop {
                interval.tick().await;
                // TODO: å®ç°é¢„å–é€»è¾‘
                debug!("ğŸš€ æ‰§è¡Œé¢„å–ä¼˜åŒ–");
            }
        });
    }

    /// è·å–å†…å­˜ç»Ÿè®¡
    pub fn get_memory_stats(&self) -> MemoryStatistics {
        let pools = self.memory_pools.lock().unwrap();
        let tracker = self.allocation_tracker.lock().unwrap();
        let monitor = self.memory_pressure_monitor.lock().unwrap();
        
        let mut pool_stats = HashMap::new();
        let mut total_allocated = 0;
        let mut total_peak = 0;
        
        for (name, pool) in pools.iter() {
            pool_stats.insert(name.clone(), PoolStatistics {
                total_allocated: pool.total_allocated,
                peak_allocated: pool.peak_allocated,
                allocation_count: pool.allocation_count,
                deallocation_count: pool.deallocation_count,
                fragmentation_ratio: pool.fragmentation_ratio,
            });
            total_allocated += pool.total_allocated;
            total_peak += pool.peak_allocated;
        }
        
        MemoryStatistics {
            total_allocated,
            peak_allocated: total_peak,
            current_pressure: monitor.current_pressure,
            fragmentation_ratio: tracker.fragmentation_analyzer.fragmentation_ratio,
            suspected_leaks: tracker.leak_detector.suspected_leaks.len(),
            pool_stats,
            cache_hit_rate: 85.0 + fastrand::f64() * 10.0, // æ¨¡æ‹Ÿç¼“å­˜å‘½ä¸­ç‡
        }
    }
}

impl<K: std::hash::Hash + Eq + Clone, V: Clone> LruCache<K, V> {
    fn new(capacity: usize) -> Self {
        Self {
            capacity,
            items: HashMap::new(),
            access_order: VecDeque::new(),
            access_counter: 0,
        }
    }
    
    fn get(&mut self, key: &K) -> Option<&V> {
        if let Some((value, _)) = self.items.get_mut(key) {
            self.access_counter += 1;
            self.access_order.push_back((key.clone(), self.access_counter));
            Some(value)
        } else {
            None
        }
    }
    
    fn put(&mut self, key: K, value: V) {
        if self.items.len() >= self.capacity {
            self.evict_lru();
        }
        
        self.access_counter += 1;
        self.items.insert(key.clone(), (value, self.access_counter));
        self.access_order.push_back((key, self.access_counter));
    }
    
    fn evict_lru(&mut self) {
        if let Some((lru_key, _)) = self.access_order.pop_front() {
            self.items.remove(&lru_key);
        }
    }
}

/// å†…å­˜ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryStatistics {
    pub total_allocated: usize,
    pub peak_allocated: usize,
    pub current_pressure: f64,
    pub fragmentation_ratio: f64,
    pub suspected_leaks: usize,
    pub pool_stats: HashMap<String, PoolStatistics>,
    pub cache_hit_rate: f64,
}

/// æ± ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PoolStatistics {
    pub total_allocated: usize,
    pub peak_allocated: usize,
    pub allocation_count: u64,
    pub deallocation_count: u64,
    pub fragmentation_ratio: f64,
}

/// å†…å­˜é”™è¯¯
#[derive(Debug, thiserror::Error)]
pub enum MemoryError {
    #[error("å†…å­˜æ± ä¸å­˜åœ¨: {0}")]
    PoolNotFound(String),
    #[error("å†…å­˜å—ä¸å­˜åœ¨: {0}")]
    ChunkNotFound(u64),
    #[error("å¤§å°è¶…è¿‡å—å¤§å°é™åˆ¶: {0} > {1}")]
    SizeExceedsChunkSize(usize, usize),
    #[error("å†…å­˜ä¸è¶³")]
    OutOfMemory,
    #[error("å‹ç¼©å¤±è´¥: {0}")]
    CompressionError(String),
    #[error("è§£å‹å¤±è´¥: {0}")]
    DecompressionError(String),
    #[error("NUMAåˆ†é…å¤±è´¥: {0}")]
    NumaAllocationError(String),
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_memory_optimizer_creation() {
        let config = MemoryConfig::default();
        let optimizer = MemoryOptimizer::new(config);
        let stats = optimizer.get_memory_stats();
        assert_eq!(stats.total_allocated, 0);
    }

    #[tokio::test]
    async fn test_pool_allocation() {
        let optimizer = MemoryOptimizer::new(MemoryConfig::default());
        
        let chunk_id = optimizer.allocate_from_pool("small", 32).unwrap();
        // chunk_id is u64, check it's a valid ID
        assert!(chunk_id > 0);
        
        optimizer.deallocate_from_pool("small", chunk_id).unwrap();
    }

    #[test]
    fn test_lru_cache() {
        let mut cache = LruCache::new(2);
        cache.put("key1".to_string(), "value1".to_string());
        cache.put("key2".to_string(), "value2".to_string());
        
        assert_eq!(cache.get(&"key1".to_string()), Some(&"value1".to_string()));
        
        // æ·»åŠ ç¬¬ä¸‰ä¸ªé¡¹åº”è¯¥evictæœ€å°‘ä½¿ç”¨çš„
        cache.put("key3".to_string(), "value3".to_string());
        // ç”±äºkey1è¢«è®¿é—®è¿‡ï¼Œkey2åº”è¯¥è¢«evict
        assert_eq!(cache.get(&"key2".to_string()), None); // key2åº”è¯¥è¢«evictäº†
    }
}